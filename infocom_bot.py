import os
import re
import sys
import json
import traceback
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse, parse_qs

# ë””ìŠ¤ì½”ë“œ ì›¹í›… (í™˜ê²½ë³€ìˆ˜ ìš°ì„ , ì—†ìœ¼ë©´ config.json ë°±ì—…)
WEBHOOK = os.getenv("DISCORD_WEBHOOK_INFOCOM")
if not WEBHOOK:
    try:
        with open("config.json", encoding="utf-8") as f:
            WEBHOOK = json.load(f)["DISCORD_WEBHOOK_INFOCOM"]
    except Exception:
        sys.exit("âŒ DISCORD_WEBHOOK_INFOCOM ì‹œí¬ë¦¿(ë˜ëŠ” config.json) ëˆ„ë½")

# ê³µì§€ ëª©ë¡ URL ë° ìƒìˆ˜
BASE       = "https://infocom.ssu.ac.kr"
LIST_URL   = f"{BASE}/kor/notice/undergraduate.php"
ID_FILE    = "last_infocom_id.txt"
HEADERS    = {"User-Agent": "Mozilla/5.0"}  # ê°„ë‹¨í•œ ë´‡ ì°¨ë‹¨ íšŒí”¼ìš©
TIMEOUT    = 15

def read_last_id():
    """ë§ˆì§€ë§‰ìœ¼ë¡œ ì „ì†¡í•œ ê¸€ì˜ idxë¥¼ ì½ëŠ”ë‹¤."""
    try:
        return open(ID_FILE, encoding="utf-8").read().strip()
    except FileNotFoundError:
        return None

def write_last_id(idx: str):
    """ë§ˆì§€ë§‰ìœ¼ë¡œ ì „ì†¡í•œ ê¸€ì˜ idxë¥¼ ê¸°ë¡í•œë‹¤."""
    with open(ID_FILE, "w", encoding="utf-8") as f:
        f.write(str(idx))

def extract_idx(href: str) -> str | None:
    """ê¸€ ë§í¬ì˜ ì¿¼ë¦¬ì—ì„œ idx ìˆ«ìë¥¼ ë½‘ì•„ë‚¸ë‹¤."""
    try:
        qs = parse_qs(urlparse(href).query)
        if "idx" in qs and qs["idx"]:
            return str(qs["idx"][0])
    except Exception:
        pass
    m = re.search(r"[?&]idx=(\d+)", href or "")
    return m.group(1) if m else None

def fetch_new_posts(last_id: str | None):
    """
    ëª©ë¡ í˜ì´ì§€ì—ì„œ ì•µì»¤ë“¤ì„ í›‘ì–´ (idx, title, link) ë¦¬ìŠ¤íŠ¸ë¥¼ ë§Œë“ ë‹¤.
    last_id ì´ì „ ê¸€ì„ ë§Œë‚˜ë©´ ì¤‘ë‹¨í•˜ê³ , ìƒˆ ê¸€ë“¤ë§Œ ë°˜í™˜í•œë‹¤.
    ë°˜í™˜ì€ ì˜¤ë˜ëœ ê¸€ë¶€í„° ì „ì†¡í•  ìˆ˜ ìˆê²Œ ì—­ìˆœìœ¼ë¡œ ì •ë ¬í•œë‹¤.
    """
    try:
        resp = requests.get(LIST_URL, headers=HEADERS, timeout=TIMEOUT)
        resp.raise_for_status()
        html = resp.text
    except Exception:
        traceback.print_exc()
        sys.exit("ğŸš« ëª©ë¡ í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨")

    soup = BeautifulSoup(html, "html.parser")

    # ì´ ì‚¬ì´íŠ¸ëŠ” ê°™ì€ í˜ì´ì§€ì—ì„œ ë³´ê¸°(view)ë¡œ ì—°ê²°ë˜ëŠ” í˜•íƒœì´ë©°,
    # hrefì— undergraduate.phpì™€ idx íŒŒë¼ë¯¸í„°ê°€ í¬í•¨ë˜ì–´ ìˆìŒ.
    anchors = soup.select('a[href*="undergraduate.php"][href*="idx="]')
    if not anchors:
        # êµ¬ì¡° ë³€í™” ì§„ë‹¨ì„ ìœ„í•´ ë””ë²„ê·¸ íŒŒì¼ ì €ì¥
        with open("infocom_debug.html", "w", encoding="utf-8") as f:
            f.write(html)
        sys.exit("ğŸš« ê³µì§€ ë§í¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤(ì…€ë ‰í„° ë¶ˆì¼ì¹˜). infocom_debug.html í™•ì¸")

    new_posts = []
    for a in anchors:
        href = a.get("href")
        link = urljoin(BASE, href)
        idx  = extract_idx(link)
        if not idx:
            continue
        if last_id and idx == last_id:
            break  # ë§ˆì§€ë§‰ìœ¼ë¡œ ë³¸ ê¸€ì— ë„ë‹¬ â†’ ê·¸ ì´ì „ì€ ì´ë¯¸ ì „ì†¡ë¨

        # ì œëª©ì€ ì•µì»¤ í…ìŠ¤íŠ¸ ê¸°ì¤€ìœ¼ë¡œ ì¶”ì¶œ
        title = a.get_text(" ", strip=True)
        if not title:
            # ë¶€ëª¨ ìš”ì†Œì— í…ìŠ¤íŠ¸ê°€ ìˆì„ ê°€ëŠ¥ì„±ê¹Œì§€ ê³ ë ¤
            title = a.find_parent().get_text(" ", strip=True) if a.find_parent() else "ì œëª© ì—†ìŒ"

        new_posts.append((idx, title, link))

    # ìµœì‹  â†’ ì˜¤ë˜ëœ ìˆœìœ¼ë¡œ ìˆ˜ì§‘ë˜ì—ˆì„ ìˆ˜ ìˆìœ¼ë‹ˆ ì „ì†¡ì€ ì˜¤ë˜ëœ ê²ƒë¶€í„°
    new_posts.reverse()
    return new_posts

def send_to_discord(msg: str):
    """ë””ìŠ¤ì½”ë“œ ì›¹í›…ìœ¼ë¡œ ë©”ì‹œì§€ ì „ì†¡."""
    requests.post(WEBHOOK, json={"content": msg}, timeout=10)

def main():
    """ë©”ì¸ ë£¨í‹´: ìƒˆ ê¸€ë“¤ì„ ëª¨ë‘ ì „ì†¡í•˜ê³  ë§ˆì§€ë§‰ idx ê°±ì‹ ."""
    last_id = read_last_id()
    posts = fetch_new_posts(last_id)

    if not posts:
        print("â¸ ìƒˆ ê³µì§€ ì—†ìŒ")
        return

    for idx, title, link in posts:
        send_to_discord(f"ğŸ”” ì „ìì •ë³´ê³µí•™ë¶€ ìƒˆ ê³µì§€\n{title}\n{link}")
        write_last_id(idx)
        print(f"âœ… ì „ì†¡ ì™„ë£Œ: {idx} {title}")

if __name__ == "__main__":
    main()
