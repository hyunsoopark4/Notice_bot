# me_bot.py  â”€â”€ í•œêµ­ í”„ë¡ì‹œ ë‹¤ì¤‘ ì‹œë„ ë²„ì „
import os, re, sys, time, random, requests
from bs4 import BeautifulSoup
from datetime import datetime

WEBHOOK  = os.getenv("DISCORD_WEBHOOK_ME")
LIST_URL = "https://me.ssu.ac.kr/notice/notice01.php"
ID_FILE  = "last_me_id.txt"

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; rv:124.0) Gecko/20100101 Firefox/124.0"
}
TIMEOUT = 15        # í”„ë¡ì‹œ í’ˆì§ˆ ê³ ë ¤í•´ 15ì´ˆ
TRIES   = 2         # í”„ë¡ì‹œë³„ ì¬ì‹œë„ 2íšŒ

# âš ï¸ ë¬´ë£Œ ê³µê°œ í•œêµ­ í”„ë¡ì‹œ ìƒ˜í”Œ(2025-07 ê°±ì‹ ) 8ê°œ
PROXIES = [
    "http://146.56.43.43:3128",
    "http://146.56.43.1:80",
    "http://61.100.180.198:8080",
    "http://121.138.83.94:3128",
    "http://58.180.224.188:80",
    "http://210.179.83.199:3128",
    "http://58.230.28.92:80",
    "http://152.70.252.193:3128",
]

def parse_date(txt: str) -> datetime:
    return datetime.strptime(txt.strip().replace(".", "-"), "%Y-%m-%d")

def fetch_html():
    random.shuffle(PROXIES)  # ë§¤ ì‹¤í–‰ë§ˆë‹¤ ìˆœì„œ ì„ê¸°
    for px in PROXIES:
        for attempt in range(1, TRIES + 1):
            try:
                r = requests.get(
                    LIST_URL,
                    headers=HEADERS,
                    timeout=TIMEOUT,
                    proxies={"http": px, "https": px},
                )
                if r.status_code == 200 and "<html" in r.text.lower():
                    print(f"âœ…  í”„ë¡ì‹œ {px} ì„±ê³µ")
                    return r.text
            except requests.exceptions.RequestException as e:
                print(f"âš ï¸  {px} ì‹¤íŒ¨({attempt}/{TRIES}) â€“ {e}")
        print(f"ğŸ›‘  {px} í¬ê¸°, ë‹¤ìŒ í”„ë¡ì‹œë¡œâ€¦")
    return None

def get_latest():
    html = fetch_html()
    if not html:
        return None, None, None

    soup = BeautifulSoup(html, "html.parser")
    latest_link, latest_dt = None, datetime.min

    for tr in soup.select("tr"):
        date_td = tr.find("td", string=re.compile(r"\d{4}.\d{2}.\d{2}"))
        link_a  = tr.find("a", href=lambda h: h and "wr_id=" in h)
        if not (date_td and link_a):
            continue
        try:
            cur_dt = parse_date(date_td.get_text())
        except ValueError:
            continue
        if cur_dt >= latest_dt:
            latest_dt, latest_link = cur_dt, link_a

    if latest_link:
        link = latest_link["href"]
        if link.startswith("/"):
            link = "https://me.ssu.ac.kr" + link
        title = latest_link.get_text(strip=True)
        wid = re.search(r"wr_id=(\d+)", link).group(1)
        return wid, title, link
    return None, None, None

def read_last():
    try:
        return open(ID_FILE).read().strip()
    except FileNotFoundError:
        return None

def write_last(wid):
    with open(ID_FILE, "w") as f:
        f.write(wid)

def send(msg):
    requests.post(WEBHOOK, json={"content": msg}, timeout=10)

def main():
    if not WEBHOOK:
        sys.exit("âŒ DISCORD_WEBHOOK_ME ì‹œí¬ë¦¿ì´ ì—†ìŠµë‹ˆë‹¤")

    wid, title, link = get_latest()
    if not wid:
        print("ğŸš« ëª¨ë“  í”„ë¡ì‹œ ì‹¤íŒ¨ â€“ ì´ë²ˆ ì£¼ê¸° ìŠ¤í‚µ")
        return

    if wid == read_last():
        print("â¸  ìƒˆ ê¸€ ì—†ìŒ")
        return

    send(f"ğŸ”§ **ê¸°ê³„ê³µí•™ë¶€ ìƒˆ ê³µì§€**\n{title}\n{link}")
    write_last(wid)
    print("âœ… ìƒˆ ê³µì§€ ì „ì†¡ ì™„ë£Œ")

if __name__ == "__main__":
    main()
