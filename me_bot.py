# me_bot.py
# Cloudflare Worker(í•œêµ­ IP) í”„ë¡ì‹œ URLë§Œ í˜¸ì¶œí•´ì„œ
# ê¸°ê³„ê³µí•™ë¶€ ìµœì‹  ê³µì§€ë¥¼ ê°€ì ¸ì˜¤ê³ , ìƒˆ ê¸€ì¼ ë•Œë§Œ ë””ìŠ¤ì½”ë“œë¡œ ì•Œë¦¼.

import os, re, sys, requests
from bs4 import BeautifulSoup
from datetime import datetime

# â”€â”€ í™˜ê²½ë³€ìˆ˜ / ìƒìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
WEBHOOK  = os.getenv("DISCORD_WEBHOOK_ME")                 # ë””ìŠ¤ì½”ë“œ ì›¹í›…
# â‘  ì•„ë˜ URLì„ **ë³¸ì¸ ì›Œì»¤ ì£¼ì†Œ**ë¡œ ë°”ê¿” ì£¼ì„¸ìš”
LIST_URL = (
    "https://me-proxy.<subdomain>.workers.dev/"
    "?url=https://me.ssu.ac.kr/notice/notice01.php"
)
ID_FILE  = "last_me_id.txt"
HEADERS  = {"User-Agent": "Mozilla/5.0"}

TIMEOUT  = 15      # ì´ˆ

# â”€â”€ í—¬í¼ í•¨ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def parse_date(txt: str) -> datetime:
    return datetime.strptime(txt.strip().replace(".", "-"), "%Y-%m-%d")

def get_latest():
    """ê²Œì‹œ ë‚ ì§œê°€ ê°€ì¥ ìµœì‹ ì¸ ê¸€ 1ê±´(wr_id, ì œëª©, ë§í¬) ë°˜í™˜"""
    r = requests.get(LIST_URL, headers=HEADERS, timeout=TIMEOUT)
    if r.status_code != 200:
        print(f"ğŸš« Worker ì‘ë‹µ ì˜¤ë¥˜ {r.status_code}")
        return None, None, None

    soup = BeautifulSoup(r.text, "html.parser")
    latest_link, latest_dt = None, datetime.min

    for tr in soup.select("tr"):
        date_td = tr.find("td", string=re.compile(r"\d{4}.\d{2}.\d{2}"))
        link_a  = tr.find("a", href=lambda h: h and "wr_id=" in h)
        if not (date_td and link_a):
            continue
        try:
            cur_dt = parse_date(date_td.get_text())
        except ValueError:
            continue
        if cur_dt >= latest_dt:
            latest_dt, latest_link = cur_dt, link_a

    if not latest_link:
        return None, None, None

    link = latest_link["href"]
    if link.startswith("/"):
        link = "https://me.ssu.ac.kr" + link
    title = latest_link.get_text(strip=True)
    wid = re.search(r"wr_id=(\d+)", link).group(1)
    return wid, title, link

def read_last():
    try:
        return open(ID_FILE).read().strip()
    except FileNotFoundError:
        return None

def write_last(wid):
    with open(ID_FILE, "w") as f:
        f.write(wid)

def send(msg):
    requests.post(WEBHOOK, json={"content": msg}, timeout=10)

# â”€â”€ ë©”ì¸ ë£¨í‹´ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    if not WEBHOOK:
        sys.exit("âŒ DISCORD_WEBHOOK_ME ì‹œí¬ë¦¿ì´ ì—†ìŠµë‹ˆë‹¤")

    wid, title, link = get_latest()
    if not wid:
        print("â¸  ê¸€ì„ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤ â€“ ë‹¤ìŒ ì£¼ê¸° ëŒ€ê¸°")
        return

    if wid == read_last():
        print("â¸  ìƒˆ ê¸€ ì—†ìŒ")
        return

    send(f"ğŸ”§ **ê¸°ê³„ê³µí•™ë¶€ ìƒˆ ê³µì§€**\n{title}\n{link}")
    write_last(wid)
    print("âœ… ìƒˆ ê³µì§€ ì „ì†¡ ì™„ë£Œ")

if __name__ == "__main__":
    main()
