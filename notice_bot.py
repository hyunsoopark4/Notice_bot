import os, sys, json, re, requests, textwrap, traceback
from bs4 import BeautifulSoup

# â”€â”€ í™˜ê²½ ë³€ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
WEBHOOK_URL = os.getenv("DISCORD_WEBHOOK_URL")
OPENAI_KEY  = os.getenv("OPENAI_API_KEY")     # â† ìƒˆë¡œ ì¶”ê°€
if not WEBHOOK_URL:
    try:
        with open("config.json", encoding="utf-8") as f:
            WEBHOOK_URL = json.load(f)["DISCORD_WEBHOOK_URL"]
    except Exception:
        sys.exit("âŒ DISCORD_WEBHOOK_URL ì„¤ì •ì´ ì—†ìŠµë‹ˆë‹¤")

if OPENAI_KEY:
    import openai
    openai.api_key = OPENAI_KEY

NOTICE_URL = "https://scatch.ssu.ac.kr/ê³µì§€ì‚¬í•­/"
LAST_FILE  = "last_notice_id.txt"
UA_HEADER  = {"User-Agent": "Mozilla/5.0"}

# â”€â”€ ìƒíƒœ íŒŒì¼ I/O â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
read_last  = lambda: open(LAST_FILE).read().strip() if os.path.exists(LAST_FILE) else None
write_last = lambda x: open(LAST_FILE, "w").write(x)

# â”€â”€ GPT ìš”ì•½ (ì—†ìœ¼ë©´ ìƒëµ) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def summarize(txt: str) -> str:
    if not OPENAI_KEY:
        return ""  # ìš”ì•½ ìƒëµ
    try:
        resp = openai.ChatCompletion.create(
            model="gpt-3.5-turbo",
            messages=[{
                "role": "user",
                "content": "ë‹¤ìŒ í•™ì‚¬ ê³µì§€ë¥¼ í•œêµ­ì–´ë¡œ ìµœëŒ€ 3ì¤„ í•µì‹¬ ìš”ì•½:\n" + txt
            }],
            max_tokens=120, temperature=0.3,
        )
        return resp.choices[0].message.content.strip()
    except Exception as e:
        print("GPT ìš”ì•½ ì‹¤íŒ¨:", e)
        return ""

# â”€â”€ ê³µì§€ ëª©ë¡ íŒŒì‹± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fetch_new_notices(last_id):
    html = requests.get(NOTICE_URL, headers=UA_HEADER, timeout=10).text
    soup = BeautifulSoup(html, "html.parser")

    posts = []
    for a in soup.select("ul.notice-lists li a"):
        link = a["href"]
        if link.startswith("/"):
            link = "https://scatch.ssu.ac.kr" + link
        m = re.search(r"[?&]num=(\d+)", link)
        nid = m.group(1) if m else link
        if nid == last_id:
            break
        title = a.get_text(" ", strip=True)

        # ë³¸ë¬¸ HTML â†’ í…ìŠ¤íŠ¸ (ìš”ì•½ìš©)
        try:
            art = requests.get(link, headers=UA_HEADER, timeout=10).text
            body = BeautifulSoup(art, "html.parser").get_text(" ", strip=True)
            body = textwrap.shorten(body, 4000)
        except Exception:
            body = ""

        posts.append((nid, title, link, body))

    return list(reversed(posts))  # ì˜¤ë˜ëœ ê²ƒë¶€í„°

# â”€â”€ ë””ìŠ¤ì½”ë“œ ì „ì†¡ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def send(content):
    requests.post(WEBHOOK_URL, json={"content": content}, timeout=10)

# â”€â”€ ë©”ì¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    last_id = read_last()
    try:
        new_posts = fetch_new_notices(last_id)
    except Exception:
        traceback.print_exc()
        sys.exit("ğŸš« ê³µì§€ íŒŒì‹± ì‹¤íŒ¨")

    if not new_posts:
        print("â¸ ìƒˆ ê¸€ ì—†ìŒ"); return

    for nid, title, link, body in new_posts:
        summary = summarize(body)
        msg = f"ğŸ“¢ **ìƒˆ í•™ì‚¬ ê³µì§€**\n{title}"
        if summary:
            msg += f"\n{summary}"
        msg += f"\n{link}"
        send(msg)
        write_last(nid)
        print(f"âœ… ì „ì†¡: {nid}")

if __name__ == "__main__":
    main()
